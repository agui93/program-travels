

### 原文
- Metadata
	- OriginalAuthor :  [Unmesh Joshi](https://twitter.com/unmeshjoshi)
	- OriginalLink : [Patterns of Distributed Systems](https://martinfowler.com/articles/patterns-of-distributed-systems/#PuttingItAllTogether-PatternSequences) 
	- OriginalPublisher :  [martinFowler平台](https://martinfowler.com/)
	- Date : 2022-03-23_星期三
	- Tag :  #distributed-system   


[Patterns of Distributed Systems](https://martinfowler.com/articles/patterns-of-distributed-systems/#PuttingItAllTogether-PatternSequences) 



### 高亮摘要

>**Consensus** refers to a set of servers which agree on stored data, 
>the order in which the data is stored and when to make that data visible to the clients.


- ==**Problems and Their Recurring Solutions**==
	- Process crashes
		- guarantee durability :  [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) 
		- guarantee  availability : store the data on multiple servers
	- Network delays
		- Problem 
			- server liveness 
			- split brain , network partition
		- Solution
			-  [HeartBeat](https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html)  
			-  [Quorum](https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html)  
			- [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html)  
				-  [High-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html) 
	- Process Pauses
		-  [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html) 
	- Unsynchronized Clocks and Ordering Events
		-  [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) 
		- [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html)
		-  [Hybrid Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html) 
		-  [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) 
		-  [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html).
	

- ==**Some topics about Distributed systems.**==
	-   Group Membership and Failure Detection
	-   Partitioning
	-   Replication and Consistency
	-   Storage
	-   Processing

- ==**Pattern Sequences**==
	- Fault Tolerant Consensus
		- Popular enterprise systems : [Zookeeper](https://zookeeper.apache.org/),   [etcd](https://etcd.io/) and [Consul](https://www.consul.io/).
		- Consensus algorithms :  [zab](https://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.html#sc_atomicBroadcast)  ,   [Raft](https://raft.github.io/)   , [Paxos](https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html) ,    [multi-paxos](https://www.youtube.com/watch?v=JEpsBg0AO6o&t=1920s)
		-   [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html) 
	- Pattern Sequence for implementing replicated log
		- durability guarantees
			- [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) 
			- [Segmented Log](https://martinfowler.com/articles/patterns-of-distributed-systems/log-segmentation.html)
			-  [Low-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/low-watermark.html)
		- Fault tolerance
			-  [Quorum](https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html) 
			-  [High-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html)  
			- [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html)
				-  [Single Socket Chan1nel](https://martinfowler.com/articles/patterns-of-distributed-systems/single-socket-channel.html)
					-  [Singular Update Queue](https://martinfowler.com/articles/patterns-of-distributed-systems/singular-update-queue.html) 
					-  [Request Pipeline](https://martinfowler.com/articles/patterns-of-distributed-systems/request-pipeline.html) 
				-  [HeartBeat](https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html) 
				-  [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html)
				-  [Follower Reads](https://martinfowler.com/articles/patterns-of-distributed-systems/follower-reads.html) 
	- Atomic Commit
		- What does atomic operation mean
		- [Two Phase Commit](https://martinfowler.com/articles/patterns-of-distributed-systems/two-phase-commit.html) 
			-  [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) 
	- Kubernetes or Kafka Control Plane
		- [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) 
			-  [Idempotent Receiver](https://martinfowler.com/articles/patterns-of-distributed-systems/idempotent-receiver.html)
			- [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html)
		- [Lease](https://martinfowler.com/articles/patterns-of-distributed-systems/time-bound-lease.html) 
		- [State Watch](https://martinfowler.com/articles/patterns-of-distributed-systems/state-watch.html) 
	- Logical Timestamp usage
		-  [Gossip Dissemination](https://martinfowler.com/articles/patterns-of-distributed-systems/gossip-dissemination.html) or a [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) for group membership and failure detection of cluster nodes.
			- The data storage uses [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) to be able to determine which values are most recent.
		- [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) 
			- [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) 
			- [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html)
			- [Hybrid Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html)
			- [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) 





### 重点摘要
- **Consensus Concept**
	- Consensus refers to a set of servers which agree on stored data, the order in which the data is stored and when to make that data visible to the clients.
- **Problems and Their Recurring Solutions**
	- Process crashes
		- A technique called [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) is used to guarantee durability.
		- Store the data on multiple servers is used to guarantee  availability in the case of single server failure.
	- Network delays
		- Problem
			- Server down
			- Split brain , network partition
		- Solution
			- A technique called [HeartBeat](https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html)  is used to detect  whether server is alive
			- A technique called [Quorum](https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html) is used to take care of the split brain issue, but not enough to give strong consistency guarantees to clients.
			-  [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html) is used in the situation that Quorum is not enough to give strong consistency guarantees to clients.
				- A [High-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html) is used to track the entry in the write ahead log that is known to have successfully replicated to a quorum of followers.
				- All the entries upto the high-water mark are made visible to the clients.
				- The leader also propagates the high-water mark to the followers.
				- So in case the leader fails and one of the followers becomes the new leader, there are no inconsistencies in what a client sees.
	- Process Pauses
		- Solution :  [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html) is used to mark and detect requests from older leaders. The generation is a number which is monotonically increasing.
	-  Unsynchronized Clocks and Ordering Events
		- [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) is used to maintain ordering of messages， track happens-before relationship between events across processes which communicate with each other. But it does not have any relation to the time of the day clock.
		-  [Hybrid Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html) uses system time along with a separate number to make sure the value increases monotonically, and can be used the same way as Lamport Clock.
		-  [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) is used to detect concurrent updates to the same value happening across a set of replicas.
		- What is [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html).
- **Pattern Sequences** 
	- Fault Tolerant Consensus
		- Popular enterprise systems : [Zookeeper](https://zookeeper.apache.org/),   [etcd](https://etcd.io/) and [Consul](https://www.consul.io/).
		- Consensus algorithms :  [zab](https://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.html#sc_atomicBroadcast)  ,   [Raft](https://raft.github.io/)   , [Paxos](https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html) ,    [multi-paxos](https://www.youtube.com/watch?v=JEpsBg0AO6o&t=1920s)
		-  When data is replicated across cluster nodes, achieving consensus on a single value is not enough.
			- All the replicas need to reach agreement on all the data.
			- To reach agreement on all the data  by executing [Paxos](https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html) multiple times while maintaining strict order.
			-  [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html) describes how basic [Paxos](https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html) can be extended to achieve this.
				- This technique is also known as [state machine replication](https://en.wikipedia.org/wiki/State_machine_replication) to achieve fault tolerance.
				- The key implementation technique used to achieve this is to replicate [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) on all the servers to have a [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html).
	- Pattern Sequence for implementing replicated log 
		- To provide durability guarantees
			- you can use the [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) pattern. 
			- The Write Ahead Log is divided into multiple segments using [Segmented Log](https://martinfowler.com/articles/patterns-of-distributed-systems/log-segmentation.html). 
			- This helps with log cleaning, which is handled by [Low-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/low-watermark.html).
		- Fault tolerance is provided by replicating the write-ahead log on multiple servers.
			- The replication among the servers is managed using the [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html)pattern 
			- and [Quorum](https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html) is used to update the [High-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html) to decide which values are visible to clients.
			- All the requests are processed in strict order
				- by using [Singular Update Queue](https://martinfowler.com/articles/patterns-of-distributed-systems/singular-update-queue.html). 
				- The order is maintained while sending the requests from leaders to followers using [Single Socket Channel](https://martinfowler.com/articles/patterns-of-distributed-systems/single-socket-channel.html). 
				- To optimize for throughput and latency over a single socket channel, [Request Pipeline](https://martinfowler.com/articles/patterns-of-distributed-systems/request-pipeline.html) can be used.
			- Followers know about availability of the leader via the [HeartBeat](https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html) received from the leader.
			- If the leader is temporarily disconnected from the cluster because of network partition, it is detected by using [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html).
			- If all the requests are served only by the leader, it might get overloaded. When the clients are read only and tolerate reading stale values, they can be served by the follower servers. [Follower Reads](https://martinfowler.com/articles/patterns-of-distributed-systems/follower-reads.html) allows handling read requests from follower servers.
	- Atomic Commit
		- What does atomic operation mean
		- [Two Phase Commit](https://martinfowler.com/articles/patterns-of-distributed-systems/two-phase-commit.html) 
			- is used to guarantee atomicity across a set of partitions.
			- To allow better throughput without using conflicting locks, two-phase-commit implementations often use [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) based storage.
	- Kubernetes or Kafka Control Plane
		-  [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) is used as a strongly consistent, fault tolerant metadata store.
			- The [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) implementation uses [Idempotent Receiver](https://martinfowler.com/articles/patterns-of-distributed-systems/idempotent-receiver.html) to ignore duplicate requests sent by cluster nodes in case of retries on network failure.
			- The Consistent Core is built with a 'Replicated Wal'
		-  [Lease](https://martinfowler.com/articles/patterns-of-distributed-systems/time-bound-lease.html) is used to implement group membership and failure detection of cluster nodes.
		- Cluster nodes use [State Watch](https://martinfowler.com/articles/patterns-of-distributed-systems/state-watch.html) to get notified when any cluster node fails or updates its metadata
	- Logical Timestamp usage
		- Various products use either a [Gossip Dissemination](https://martinfowler.com/articles/patterns-of-distributed-systems/gossip-dissemination.html) or a [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) for group membership and failure detection of cluster nodes.
		- The data storage uses [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) to be able to determine which values are most recent.
		- If a single server is responsible for updating the values or [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html) is used, then a [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) can be used as a version, in the [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html).
		- When the timestamp values need to be derived from the time of the day, a [Hybrid Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html) is used instead of a simple Lamport Clock.
		- If multiple servers are allowed to handle client requests to update the same value, a [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) is used to be able to detect concurrent writes on different cluster nodes.
- **Some topics about Distributed systems.**
	-   Group Membership and Failure Detection
	-   Partitioning
	-   Replication and Consistency
	-   Storage
	-   Processing




### 原文摘要

| Type of platform/framework   | Example                                    |
|:---------------------------- |:------------------------------------------ |
| Databases                    | Cassandra, HBase, Riak                     |
| Message Brokers              | Kafka, Pulsar                              |
| Infrastructure               | Kubernetes, Mesos, Zookeeper, etcd, Consul |
| In Memory Data/Compute Grids | Hazelcast, Pivotal Gemfire                 |
| Stateful Microservices       | Akka Actors, Axon                          |
| File Systems                 | HDFS, Ceph                                 |


- Distributed systems - An implementation perspective
	- What does it mean for a system to be distributed? There are two aspects:
		- They run on multiple servers. The number of servers in a cluster can vary from as few as three servers to a few thousand servers.
		- They manage data. So these are inherently 'stateful' systems
	- Problem and some recurring solutions
		- There are several ways in which things can go wrong when multiple servers are involved in storing data. 
		- The implementation of these systems have some recurring solutions to these problems.
		- Understanding these solutions in their general form helps in understanding the implementation of the broad spectrum of these systems and can also serve as a good guidance when new systems need to be built.
	

- Problems and Their Recurring Solutions
	- **Process crashes**
		-  Processes can crash at any time maybe due to hardware faults or software faults.
			-   It can be taken down for routine maintenance by system administrators.
			-   It can be killed doing some file IO because the disk is full and the exception is not properly handled.
			-   In cloud environments, it can be even trickier, as some unrelated events can bring the servers down.
		- Bottom line
			- If the processes are responsible for storing data, they must be designed to give a durability guarantee for the data stored on the servers.
		- Risk
			- Because flushing data to the disk is one of the most time consuming operations, not every insert or update to the storage can be flushed to disk.
			- Most databases have in-memory storage structures which are only periodically flushed to disk. This poses a risk of losing all the data if the process abruptly crashes. 
		- Durability guarantee
			- A technique called [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) is used to tackle this situation.
				- Servers store each state change as a command in an append-only file on a hard disk.
				- Appending a file is generally a very fast operation, so it can be done without impacting performance.
				- A single log, which is appended sequentially, is used to store each update.
				- At the server startup, the log can be replayed to build in memory state again.
				-  The data will not get lost even if the server abruptly crashes and then restarts.
			- Lack availability in the case of single server failure.
				- One of the obvious solutions is to store the data on multiple servers. So we can replicate the write ahead log on multiple servers.
				- When multiple servers are involved, there are a lot more failure scenarios which need to be considered.
	- **Network delays**
		- Delay
			- In the TCP/IP protocol stack, there is no upper bound on delays caused in transmitting messages across a network.
			- It can vary based on the load on the network.
		- Two problems
			- Server down
				- A particular server can not wait indefinitely to know if another server has crashed.
				- Solution
					- every server sends a [HeartBeat](https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html) message to other servers at a regular interval. 
					- If a heartbeat is missed, the server sending the heartbeat is considered crashed.
					- The heartbeat interval is small enough to make sure that it does not take a lot of time to detect server failure.
			- Split brain , or network partition
				- data center
					- There should not be two sets of servers, each considering another set to have failed, and therefore continuing to serve different sets of clients.
					- In a typical data center, servers are packed together in racks, 
					- and there are multiple racks connected by a top-of-the-rack switch.
					- There might be a tree of switches connecting one part of the data center to the other.
					- It is possible in some cases, that a set of servers can communicate with each other, but are disconnected from another set of servers. This situation is called a network partition.
				- two sets of servers
					- which are disconnected from each other
					- should not be able to make progress independently.
				- Solution
					- Quorum
						- The number of servers making the majority is called a [Quorum](https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html).
						- How to decide on the quorum?
							- That is decided based on the number of failures the cluster can tolerate.
							- if we have a cluster of five nodes, we need a quorum of three.
							- In general, if we want to tolerate `f` failures we need a cluster size of 2f + 1.
						-  It is not enough to give strong consistency guarantees to clients.
					- Leader and Followers
						- [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html) is used in the situation that Quorum is not enough to give strong consistency guarantees to clients.
						- One of the servers is elected a leader and the other servers act as followers.
						- The leader controls and coordinates the replication on the followers.
						- The leader now needs to decide, which changes should be made visible to the clients.
							- A [High-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html) is used to track the entry in the write ahead log that is known to have successfully replicated to a quorum of followers.
							- All the entries upto the high-water mark are made visible to the clients.
							- The leader also propagates the high-water mark to the followers.
							- So in case the leader fails and one of the followers becomes the new leader, there are no inconsistencies in what a client sees.
	- **Process Pauses**
		- There are a lot of reasons a process can pause.
			- For languages which support garbage collection, there can be a long garbage collection pause.
		- Leader process can pause arbitrarily
			- Problem
				- A leader with a long garbage collection pause, can be disconnected from the followers, and will continue sending messages to followers after the pause is over.
				- In the meanwhile, because followers did not receive a heartbeat from the leader, they might have elected a new leader and accepted updates from the clients.
				- If the requests from the old leader are processed as is, they might overwrite some of the updates.
				- So we need a mechanism to detect requests from out-of-date leaders.
			- Solution
				- [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html) is used to mark and detect requests from older leaders. The generation is a number which is monotonically increasing.
	- **Unsynchronized Clocks and Ordering Events**
		- Problem
			- how to maintain ordering of messages
				- Can not use system timestamps to order a set of messages
					- The main reason we can not use system clocks is that system clocks across servers are not guaranteed to be synchronized.
					- Mechanism of a time-of-the-day clock in a computer
						- A time-of-the-day clock in a computer is managed by a quartz crystal and measures time based on the oscillations of the crystal.
						- NTP
							- The clocks across a set of servers are synchronized by a service called NTP.
							- This service periodically checks a set of global time servers, and adjusts the computer clock accordingly.
							- Because this happens with communication over a network, and network delays can vary, the clock synchronization might be delayed because of a network issue.
							- This can cause server clocks to drift away from each other, and after the NTP sync happens, even move back in time.
					- Because of these issues with computer clocks, time of day is generally not used for ordering events.
		- Solution
			- [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) is used
				- [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html) is an example of that.
				- Lamport Clocks are just simple numbers, which are incremented only when some event happens in the system.
				- Lamport Clocks also track happens-before relationship between events across processes which communicate with each other.
					- In a database, the events are about writing and reading the values, so the lamport clock is incremented only when a value is written. 
					- The Lamport Clock numbers are also passed in the messages sent to other processes. 
					- The receiving process can then select the larger of the two numbers, the one it receives in the message and the one it maintains.
					-  This way Lamport Clocks also track happens-before relationship between events across processes which communicate with each other.
					- An example of this is the servers taking part in a transaction.
			- While the [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) allows ordering of events, it does not have any relation to the time of the day clock.
			- [Hybrid Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html) is used
				- The Hybrid Clock uses system time along with a separate number to make sure the value increases monotonically, and can be used the same way as Lamport Clock.
			-  [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) is used to detect conflict across a set of replicas.
				- The Lamport Clock allows determining the order of events across a set of communicating servers.
				- But it does not allow detecting concurrent updates to the same value happening across a set of replicas.
				- [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) is used to detect conflict across a set of replicas.
			- [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html).
				- The Lamport Clock or Version Vector needs to be associated with the stored values, to detect which values are stored after the other or if there are conflicts. 
				- So the servers store the values as [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html).

- Pattern Sequences :  Build a complete system, from the ground up.
	- **Fault Tolerant Consensus**
		- Popular enterprise systems : [Zookeeper](https://zookeeper.apache.org/),   [etcd](https://etcd.io/) and [Consul](https://www.consul.io/).
		- Consensus algorithms :  [zab](https://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.html#sc_atomicBroadcast)  ,   [Raft](https://raft.github.io/)   ,    [multi-paxos](https://www.youtube.com/watch?v=JEpsBg0AO6o&t=1920s)
		- Consensus refers to a set of servers which agree on stored data, the order in which the data is stored and when to make that data visible to the clients.
		-  When data is replicated across cluster nodes, achieving consensus on a single value is not enough.
			- All the replicas need to reach agreement on all the data.
			- To reach agreement on all the data  by executing [Paxos](https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html) multiple times while maintaining strict order.
			-  [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html) describes how basic [Paxos](https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html) can be extended to achieve this.
				- This technique is also known as [state machine replication](https://en.wikipedia.org/wiki/State_machine_replication) to achieve fault tolerance.
				- The key implementation technique used to achieve this is to replicate [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) on all the servers to have a [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html).
	- **Pattern Sequence for implementing replicated log** 
		- To provide durability guarantees
			- you can use the [Write-Ahead Log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html) pattern. 
			- The Write Ahead Log is divided into multiple segments using [Segmented Log](https://martinfowler.com/articles/patterns-of-distributed-systems/log-segmentation.html). 
			- This helps with log cleaning, which is handled by [Low-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/low-watermark.html).
		- Fault tolerance is provided by replicating the write-ahead log on multiple servers.
			- The replication among the servers is managed using the [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html)pattern 
			- and [Quorum](https://martinfowler.com/articles/patterns-of-distributed-systems/quorum.html) is used to update the [High-Water Mark](https://martinfowler.com/articles/patterns-of-distributed-systems/high-watermark.html) to decide which values are visible to clients.
			- All the requests are processed in strict order
				- by using [Singular Update Queue](https://martinfowler.com/articles/patterns-of-distributed-systems/singular-update-queue.html). 
				- The order is maintained while sending the requests from leaders to followers using [Single Socket Channel](https://martinfowler.com/articles/patterns-of-distributed-systems/single-socket-channel.html). 
				- To optimize for throughput and latency over a single socket channel, [Request Pipeline](https://martinfowler.com/articles/patterns-of-distributed-systems/request-pipeline.html) can be used.
			- Followers know about availability of the leader via the [HeartBeat](https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html) received from the leader.
			- If the leader is temporarily disconnected from the cluster because of network partition, it is detected by using [Generation Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/generation.html).
			- If all the requests are served only by the leader, it might get overloaded. When the clients are read only and tolerate reading stale values, they can be served by the follower servers. [Follower Reads](https://martinfowler.com/articles/patterns-of-distributed-systems/follower-reads.html) allows handling read requests from follower servers.
	- **Atomic Commit**
		- Partitioned Data 
			- Consensus algorithms are useful when multiple cluster nodes all store the same data. Often, data size is too big to store and process on a single node.
			- So data is partitioned across a set of nodes using various partitioning schemes such as [Fixed Partitions](https://martinfowler.com/articles/patterns-of-distributed-systems/fixed-partitions.html) or [Key-Range Partitions](https://martinfowler.com/articles/patterns-of-distributed-systems/key-range-partitions.html).
			- To achieve fault tolerance, each partition is also replicated across a few cluster nodes using [Replicated Log](https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html).
		- one atomic operation
			- Sometimes data across a set of partitions needs to be stored as one atomic operation.
			- To maintain atomicity, the data needs to be stored and made accessible on all the partitions or none of them.
		- two-phase-commit
			- [Two Phase Commit](https://martinfowler.com/articles/patterns-of-distributed-systems/two-phase-commit.html) is used to guarantee atomicity across a set of partitions.
			- To guarantee atomicity, two-phase-commit often needs to lock the data items involved. This can severely impact throughput, particularly when there are long running read-only operations holding locks.
			- To allow better throughput without using conflicting locks, two-phase-commit implementations often use [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) based storage.
	- **Kubernetes or Kafka Control Plane**
		-  [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) is used as a strongly consistent, fault tolerant metadata store.
			- The [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) implementation uses [Idempotent Receiver](https://martinfowler.com/articles/patterns-of-distributed-systems/idempotent-receiver.html) to ignore duplicate requests sent by cluster nodes in case of retries on network failure.
			- The Consistent Core is built with a 'Replicated Wal'
		-  [Lease](https://martinfowler.com/articles/patterns-of-distributed-systems/time-bound-lease.html) is used to implement group membership and failure detection of cluster nodes.
		- Cluster nodes use [State Watch](https://martinfowler.com/articles/patterns-of-distributed-systems/state-watch.html) to get notified when any cluster node fails or updates its metadata
	- **Logical Timestamp usage**
		- Various products use either a [Gossip Dissemination](https://martinfowler.com/articles/patterns-of-distributed-systems/gossip-dissemination.html) or a [Consistent Core](https://martinfowler.com/articles/patterns-of-distributed-systems/consistent-core.html) for group membership and failure detection of cluster nodes.
		- The data storage uses [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html) to be able to determine which values are most recent.
		- If a single server is responsible for updating the values or [Leader and Followers](https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html) is used, then a [Lamport Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html) can be used as a version, in the [Versioned Value](https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html).
		- When the timestamp values need to be derived from the time of the day, a [Hybrid Clock](https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html) is used instead of a simple Lamport Clock.
		- If multiple servers are allowed to handle client requests to update the same value, a [Version Vector](https://martinfowler.com/articles/patterns-of-distributed-systems/version-vector.html) is used to be able to detect concurrent writes on different cluster nodes.

- Some topics about Distributed systems.
	-   Group Membership and Failure Detection
	-   Partitioning
	-   Replication and Consistency
	-   Storage
	-   Processing






